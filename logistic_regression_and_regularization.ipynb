{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lineare_regression import logistic_function, logistic_hypothesis, cross_entropy_loss, squared_error_loss, cost_function_vec, gradient_descent_multi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifical dataset for logistic regression\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# class 0:\n",
    "# covariance matrix and mean\n",
    "\n",
    "cov0 = np.array([[5, -3],[-3, 3]])\n",
    "\n",
    "mean0 = np.array([2., 3])\n",
    "\n",
    "# number of data points\n",
    "m0 = 1000\n",
    "# generate m0 gaussian distributed data points with\n",
    "# mean0 and cov0.\n",
    "r0 = np.random.multivariate_normal(mean0, cov0, m0)\n",
    "\n",
    "# covariance matrix\n",
    "cov1 = np.array([[5,-3],[-3,3]])\n",
    "mean1 = np.array([1.,1])\n",
    "m1 = 1000\n",
    "r1 = np.random.multivariate_normal(mean1, cov1, m1)\n",
    "\n",
    "plt.scatter(r0[...,0], r0[...,1], c=\"b\", marker=\"o\", label=\"Klasse 0\")\n",
    "plt.scatter(r1[...,0], r1[...,1], c=\"r\", marker=\"x\", label=\"Klasse 1\")\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\")\n",
    "plt.show()\n",
    "\n",
    "x = np.concatenate((r0, r1))\n",
    "y = np.zeros(len(r0)+len(r1))\n",
    "\n",
    "# todo: is this needed?\n",
    "y[:len(r0),] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Erstellen Sie eine Pythonfunktion die, die\n",
    "# logistische Funktion berechnet.\n",
    "# Stellen Sie diese im Bereich [-5, 5] graphisch dar.\n",
    "\n",
    "x_values = np.linspace(-5, 5, 50)\n",
    "y_values = logistic_function(x_values)\n",
    "\n",
    "plt.plot(x_values, y_values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Implementieren Sie die Hypothese als Python Funktion:\n",
    "# logistic_hypothesis(theta)\n",
    "# Die Pythonfunktion soll dabei eine Funktion zurückgeben:\n",
    "# >> theta = np.array([1.1, 2.0, -.9]) \n",
    "# >> h = logistic_hypothesis(theta) \n",
    "# >> print h(X) \n",
    "# array([ -0.89896965, 0.71147926, ....\n",
    "\n",
    "theta = np.array([1.1, 2.0, -0.9])\n",
    "h = logistic_hypothesis(theta)\n",
    "\n",
    "print(h(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Implementieren Sie den Cross-Entropy-Loss und \n",
    "# den Squared-Error-Loss als Python Funktion. \n",
    "# Die Pythonfunktion soll dabei eine Funktion zurückgeben:\n",
    "\n",
    "# >> loss = cross_entropy_loss(h, X, y)\n",
    "# >> print loss(theta)\n",
    "# array([ 7.3, 9.5, ....\n",
    "# Rückgabevektor hat m-Elemente (Anzahl der Datensätze)\n",
    "\n",
    "print(\"squared error loss:\")\n",
    "loss_func = squared_error_loss(logistic_hypothesis, x, y)\n",
    "print(loss_func(theta))\n",
    "print(\"\")\n",
    "\n",
    "print(\"cross entropy loss:\")\n",
    "loss_func = cross_entropy_loss(logistic_hypothesis, x, y)\n",
    "print(loss_func(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Implementieren Sie die Kostenfunktion J als Python Funktion:\n",
    "# cost_function(X, y, h, loss)\n",
    "# zusätzlich zu X und y soll die Funktion die Hypothese h \n",
    "# und den Loss aufnehmen.\n",
    "# \n",
    "# Die Pythonfunktion soll dabei eine Funktion zurückgeben, die\n",
    "# den Parametnp.ndarrayervektor theta aufnimmt.\n",
    "\n",
    "cost_func = cost_function_vec(logistic_hypothesis, x, y, cross_entropy_loss)\n",
    "print(cost_func(theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Implementieren Sie das Gradientenabstiegsverfahren unter Benutzung der Kostenfunktion und der Hypothese.\n",
    "# 5a) Schreiben Sie eine Funktion die die Update Rules anwendet zur Berechnung der neuen theta-Werte:\n",
    "# theta = compute_new_theta(x, y, theta, alpha, hypothesis)\n",
    "# 5b) Wählen Sie Startwerte in der Umgebung des Miniums der Kostenfunktion für theta. \n",
    "# Wenden Sie iterativ die compute_new_theta Funktion an und finden Sie so ein Theta mit niedrigen Kosten.\n",
    "# Kapseln Sie dies in eine Funktion:\n",
    "# theta = gradient_descent(alpha, theta, nb_iterations, X, y)\n",
    "\n",
    "cost_y, new_theta = gradient_descent_multi(\n",
    "    x=x, \n",
    "    y=y, \n",
    "    theta=theta, \n",
    "    learning_rate=0.1, \n",
    "    iterations=3000, \n",
    "    hypothesis_creator=logistic_hypothesis,\n",
    "    loss_creator=cross_entropy_loss)\n",
    "\n",
    "cost_x = np.arange(0, len(cost_y))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"learning progress\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"costs\")\n",
    "plt.plot(cost_x, cost_y)\n",
    "plt.show()\n",
    "\n",
    "print(\"last cost value: {}\".format(cost_y[-1]))\n",
    "print(\"calculated theta: {}\".format(new_theta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Zeichen Sie die Entscheidungsebene in den Scatter-Plot der Daten\n",
    "# Hinweis: Für diese gilt: log_func(theta[0] + theta[1] * x1 + theta[2] * x2) = 0.5 oder\n",
    "\n",
    "#  theta[0] + theta[1] * x1 + theta[2] * x2 = 0\n",
    "\n",
    "x_decision = x[:, 0]\n",
    "y_decision = (-new_theta[0] - new_theta[1] * x_decision) / new_theta[2]\n",
    "\n",
    "plt.scatter(r0[...,0], r0[...,1], c='b', marker='o', label=\"Klasse 0\")\n",
    "plt.scatter(r1[...,0], r1[...,1], c='r', marker='x', label=\"Klasse 1\")\n",
    "plt.plot(x_decision, y_decision)\n",
    "plt.xlabel(\"x0\")\n",
    "plt.ylabel(\"x1\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Berechnen Sie den Klassifikationsfehler, d.h. der Anteil\n",
    "# der falsch-klassifizierten Datensätze:\n",
    "# Klassifikationsfehler = Anzahl der falsch-klassifizierten Datensätze / Anzahl der Datensätze\n",
    "\n",
    "h = logistic_hypothesis(new_theta)\n",
    "y_predicted = h(x)\n",
    "\n",
    "y_class_0 = np.less(y, 0.5)\n",
    "y_class_0_predicted = np.less(y_predicted, 0.5)\n",
    "\n",
    "num_wrong = np.logical_xor(y_class_0, y_class_0_predicted).sum()\n",
    "class_err = num_wrong / len(y)\n",
    "\n",
    "print(\"num of false classified values: {}\".format(num_wrong))\n",
    "print(\"classification error: {}\".format(class_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Erweitern Sie die Kostenfunktion um den Regulierungsterm lambda_reg\n",
    "\n",
    "cost_func = cost_function_vec(logistic_hypothesis, x, y, cross_entropy_loss, 0.0)\n",
    "costs_without_reg = cost_func(theta)\n",
    "\n",
    "cost_func = cost_function_vec(logistic_hypothesis, x, y, cross_entropy_loss, 10.0)\n",
    "costs_with_reg = cost_func(theta)\n",
    "\n",
    "print(\"costs without regularization: {}\".format(costs_without_reg))\n",
    "print(\"costs with regularization: {}\".format(costs_with_reg))\n",
    "print(\"cost difference {}\".format(costs_with_reg - costs_without_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Erweitern Sie den Gradientenabstieg um den Regulierungsterm lambda_reg\n",
    "\n",
    "cost_y, new_theta = gradient_descent_multi(\n",
    "    x=x, \n",
    "    y=y, \n",
    "    theta=theta, \n",
    "    learning_rate=0.1, \n",
    "    iterations=3000, \n",
    "    hypothesis_creator=logistic_hypothesis,\n",
    "    lambda_reg=0.01)\n",
    "\n",
    "cost_x = np.arange(0, len(cost_y))\n",
    "\n",
    "plt.figure()\n",
    "plt.title(\"learning progress\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"costs\")\n",
    "plt.plot(cost_x, cost_y)\n",
    "plt.show()\n",
    "\n",
    "print(\"last cost value: {}\".format(cost_y[-1]))\n",
    "print(\"calculated theta: {}\".format(new_theta))\n",
    "\n",
    "h = logistic_hypothesis(new_theta)\n",
    "y_predicted = h(x)\n",
    "\n",
    "y_class_0 = np.less(y, 0.5)\n",
    "y_class_0_predicted = np.less(y_predicted, 0.5)\n",
    "\n",
    "num_wrong = np.logical_xor(y_class_0, y_class_0_predicted).sum()\n",
    "class_err = num_wrong / len(y)\n",
    "\n",
    "print(\"num of false classified values: {}\".format(num_wrong))\n",
    "print(\"classification error: {}\".format(class_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
